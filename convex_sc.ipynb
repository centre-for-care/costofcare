{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from SyntheticControlMethods import Synth\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KDTree\n",
    "import cvxpy as cvx\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_unit= '81959733'\n",
    "treated_period = 2013\n",
    "id_var = 'pidp'\n",
    "time_var = 'year'\n",
    "outcome_var = 'ind_inc_deflated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                float64\n",
       "pidp                 object\n",
       "asian               float64\n",
       "black               float64\n",
       "dvage               float64\n",
       "ind_inc_deflated    float64\n",
       "low                 float64\n",
       "mastat_recoded      float64\n",
       "middle              float64\n",
       "mixed               float64\n",
       "other               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./test_data.csv', index_col=0)\n",
    "df['pidp'] = df['pidp'].astype(str)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_u = df[df.pidp==treated_unit].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_u = df[df.pidp!=treated_unit].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = random.sample(c_u.pidp.unique().tolist(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df[df.pidp.isin(selection)]\n",
    "data = pd.concat([sample,t_u], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.39131D+06    |proj g|=  8.88889D-01\n",
      "\n",
      "At iterate    3    f=  1.50136D+06    |proj g|=  9.25140D-01\n",
      "\n",
      "At iterate    6    f=  1.50107D+06    |proj g|=  4.42378D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9      6      8     12     0     0   4.424D-01   1.501D+06\n",
      "  F =   1501069.1430513542     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38383D+06    |proj g|=  9.97569D-01\n",
      "  ys=-8.763E+00  -gs= 2.342E+03 BFGS update SKIPPED\n",
      "\n",
      "At iterate    3    f=  1.28220D+05    |proj g|=  9.26967D-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " Bad direction in the line search;\n",
      "   refresh the lbfgs memory and restart the iteration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    6    f=  1.28033D+05    |proj g|=  8.82161D-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " Bad direction in the line search;\n",
      "   refresh the lbfgs memory and restart the iteration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    9    f=  1.20181D+05    |proj g|=  1.00000D+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/code/costofcare/lib64/python3.11/site-packages/scipy/optimize/_numdiff.py:576: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x) - f0\n",
      "\n",
      " Bad direction in the line search;\n",
      "   refresh the lbfgs memory and restart the iteration.\n",
      "\n",
      " Line search cannot locate an adequate point after MAXLS\n",
      "  function and gradient evaluations.\n",
      "  Previous x, f and g restored.\n",
      " Possible causes: 1 error in function or gradient evaluation;\n",
      "                  2 rounding error dominate computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9     12    175     38     1     5   7.825D-01   1.186D+05\n",
      "  F =   118637.52482522048     \n",
      "\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH                              \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.47129D+06    |proj g|=  9.99925D-01\n",
      "\n",
      "At iterate    3    f=  1.76680D+06    |proj g|=  4.49762D-01\n",
      "\n",
      "At iterate    6    f=  1.50108D+06    |proj g|=  9.99996D-01\n",
      "\n",
      "At iterate    9    f=  1.50107D+06    |proj g|=  0.00000D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9      9     15     17     0     4   0.000D+00   1.501D+06\n",
      "  F =   1501069.1430513498     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.53675D+06    |proj g|=  9.94196D-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/code/costofcare/lib64/python3.11/site-packages/cvxpy/problems/problem.py:1403: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    3    f=  1.17528D+05    |proj g|=  9.29442D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9      3     13     10     0     5   9.294D-01   1.175D+05\n",
      "  F =   117527.82333259824     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.44709D+06    |proj g|=  9.76605D-01\n",
      "\n",
      "At iterate    3    f=  1.50107D+06    |proj g|=  2.49375D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9      4     11     10     0     0   2.494D-01   1.501D+06\n",
      "  F =   1501069.1430513726     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.24947D+06    |proj g|=  9.51777D-01\n",
      "\n",
      "At iterate    3    f=  1.50107D+06    |proj g|=  2.48963D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9      4     10     10     0     0   2.490D-01   1.501D+06\n",
      "  F =   1501069.1430513507     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.52043D+06    |proj g|=  9.18733D-01\n",
      "\n",
      "At iterate    3    f=  1.50108D+06    |proj g|=  8.87464D-01\n",
      "\n",
      "At iterate    6    f=  1.50107D+06    |proj g|=  1.16415D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9      6      8     10     0     0   1.164D-01   1.501D+06\n",
      "  F =   1501069.1430513505     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  8.96850D+06    |proj g|=  9.81186D-01\n",
      "\n",
      "At iterate    3    f=  1.50107D+06    |proj g|=  2.49483D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9      5     12     11     0     1   4.657D-02   1.501D+06\n",
      "  F =   1501069.1430513496     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.88590D+06    |proj g|=  9.08488D-01\n",
      "\n",
      "At iterate    3    f=  1.51490D+06    |proj g|=  9.28287D-01\n",
      "\n",
      "At iterate    6    f=  1.50107D+06    |proj g|=  9.16250D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9      7      9     11     0     0   1.863D-01   1.501D+06\n",
      "  F =   1501069.1430513507     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.81832D+06    |proj g|=  9.50752D-01\n",
      "\n",
      "At iterate    3    f=  1.50107D+06    |proj g|=  2.44348D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9      4     10      8     0     0   2.095D-01   1.501D+06\n",
      "  F =   1501069.1430513498     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    }
   ],
   "source": [
    "sc = Synth(dataset=data, outcome_var=outcome_var, id_var=id_var, time_var=time_var, treatment_period=treated_period, treated_unit=treated_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_control_matrices_b(data_object, k_n):\n",
    "    data = data_object['data'].copy()\n",
    "    ncol = data.shape[1] - 1\n",
    "    sample_weights = data_object['weight'].copy()\n",
    "    data.index.names = ['var', 'year']\n",
    "    t_time = data_object['treat_time']\n",
    "    target_var = data_object['target_var']\n",
    "    data.index = data.index.map(lambda idx: (idx[0], idx[1] - t_time))\n",
    "    sample_weights.index = sample_weights.index - t_time\n",
    "    data = data.sort_index(ascending=True).copy()\n",
    "    df_T0 = data.loc[pd.IndexSlice[:, :-1], :]\n",
    "    if ncol < k_n:\n",
    "        k_n = ncol\n",
    "    try:\n",
    "        kdt = KDTree(df_T0.T, leaf_size=30, metric='euclidean')\n",
    "    except ValueError:\n",
    "        return None\n",
    "    idx = kdt.query(df_T0.T, k=k_n, return_distance=False)[0, :]\n",
    "    select_data = data.iloc[:, idx]\n",
    "    select_df_T0 = df_T0.iloc[:, idx]\n",
    "    c_out_all = np.array([x for x in select_data.iloc[:,1:].loc[[target_var]].values])\n",
    "    c_out = np.array([x for x in select_df_T0.iloc[:,1:].loc[[target_var]].values])\n",
    "    l_means = select_df_T0.iloc[:,1:].sort_index().groupby(level='var').mean()\n",
    "    c_cov_mean = np.array([x for x in l_means.values])\n",
    "    return c_out_all, c_out, c_cov_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_unit= '81959733'\n",
    "treated_period = 2014\n",
    "id_var = 'pidp'\n",
    "time_var = 'year'\n",
    "outcome_var = 'ind_inc_deflated'\n",
    "n_covariates = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pkl', 'rb') as file:\n",
    "    data_object = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_u = data[data.pidp == '81959733'].pivot(index='pidp', columns='year').T\n",
    "c_u = data[data.pidp != '81959733'].pivot(index='pidp', columns='year').T\n",
    "sample = pd.concat([t_u, c_u], axis=1, join=\"inner\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_object['data'] = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, data_object):\n",
    "        self.data_object = data_object\n",
    "        self.data = self.data_object['data'].copy()\n",
    "        self.sample_weights = self.data_object['weight'].copy()\n",
    "        self.data.index.names = ['var', 'year']\n",
    "        self.t_time = self.data_object['treat_time']\n",
    "        self.target_var = self.data_object['target_var']\n",
    "        self.n_covariates = len(self.data.index.get_level_values(0).unique())\n",
    "        self.min_loss = float(\"inf\")\n",
    "        self.random_seed = 0\n",
    "        self.rng = np.random.default_rng(self.random_seed)\n",
    "        self.steps = 8\n",
    "        self.w = None\n",
    "        self.v = None\n",
    "        self.random_seed = 0\n",
    "        self.control_outcome_all = None\n",
    "        self.control_outcome = None\n",
    "        self.unscaled_control_covariates = None\n",
    "        self.synth_outcome = None\n",
    "        self.synth_covariates = None\n",
    "        self.unscaled_treated_covariates = None\n",
    "        self.treated_outcome_all = None\n",
    "        self.treated_outcome = None\n",
    "        self.treated_covariates = None\n",
    "        self.control_covariates = None\n",
    "        self.get_control_matrices()\n",
    "        self.get_treat_matrices()\n",
    "        self.rescale_covariate_variance()\n",
    "    \n",
    "    def get_control_matrices(self):\n",
    "        self.control_outcome_all = np.array([x for x in self.data.iloc[:,1:].loc[[self.target_var]].values])\n",
    "        self.control_outcome = np.array([x for x in self.data.iloc[:,1:].loc[[self.target_var]].loc[(slice(None), slice(None, self.t_time-1)), :].values])\n",
    "        l_means = self.data.iloc[:,1:].sort_index().loc[(slice(None), slice(None, self.t_time-1)), :].groupby(level='var').mean()\n",
    "        self.unscaled_control_covariates = np.array([x for x in l_means.values])\n",
    "\n",
    "    def get_treat_matrices(self):\n",
    "        self.treated_outcome_all = np.array([[x] for x in self.data.iloc[:,0][self.target_var].values])\n",
    "        self.treated_outcome = np.array([[x] for x in self.data.iloc[:,0][self.target_var][:self.t_time-1].values])\n",
    "        l_means = self.data.iloc[:,:1].sort_index().loc[(slice(None), slice(None, self.t_time-1)), :].groupby(level='var').mean().mean(axis=1)\n",
    "        self.unscaled_treated_covariates = np.array([[x] for x in l_means.values])\n",
    "\n",
    "    def rescale_covariate_variance(self):\n",
    "        big_dataframe = np.concatenate((self.unscaled_treated_covariates, self.unscaled_control_covariates), axis=1)\n",
    "        big_dataframe /= np.apply_along_axis(np.std, 0, big_dataframe)\n",
    "        self.treated_covariates = big_dataframe[:,0].reshape(self.n_covariates, 1)\n",
    "        self.control_covariates = big_dataframe[:,1:]\n",
    "    \n",
    "    def optimize(self):\n",
    "\n",
    "        args = (self.treated_outcome,\n",
    "                self.treated_covariates,\n",
    "                self.control_outcome,\n",
    "                self.control_covariates\n",
    "                )\n",
    "        \n",
    "        for step in range(self.steps):\n",
    "            if step == 0:\n",
    "                v_0 = np.full(self.n_covariates, 1/self.n_covariates)\n",
    "            else:\n",
    "                v_0 = self.rng.dirichlet(np.ones(self.n_covariates), size=1)\n",
    "\n",
    "            bnds = tuple((0,1) for _ in range(self.n_covariates))\n",
    "            self.res = minimize(self.total_loss, v_0,  args=(args),\n",
    "                                method='L-BFGS-B', bounds=bnds,\n",
    "                                options={'gtol': 1e-8,'disp':3, 'iprint':3})\n",
    "            self.res_x = self.res.x\n",
    "    \n",
    "    def total_loss(self,\n",
    "                   v_0,\n",
    "                   treated_outcome,\n",
    "                   treated_covariates,\n",
    "                   control_outcome,\n",
    "                   control_covariates):\n",
    "        \n",
    "        n_controls = control_outcome.shape[1]\n",
    "        V = np.diag(v_0)\n",
    "        w = cvx.Variable((n_controls, 1), nonneg=True)\n",
    "        treated_synth_difference = cvx.sum(V @ cvx.square(treated_covariates.T - control_covariates @ w))\n",
    "        objective = cvx.Minimize(treated_synth_difference)\n",
    "        constraints = [cvx.sum(w) == 1]\n",
    "        problem = cvx.Problem(objective, constraints)\n",
    "        \n",
    "        try:\n",
    "            result = problem.solve(verbose=False)\n",
    "            loss = (treated_outcome - control_outcome @ w.value).T @ (treated_outcome - control_outcome @ w.value)\n",
    "        except:\n",
    "            return float(np.inf)\n",
    "\n",
    "        if loss < self.min_loss:\n",
    "            self.min_loss = loss\n",
    "            self.w = w.value\n",
    "            self.v = np.diagonal(V) / np.sum(np.diagonal(V))\n",
    "            self.synth_outcome = self.w.T @ self.control_outcome_all.T\n",
    "            self.synth_covariates = self.control_covariates @ self.w\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Optimizer(data_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.48899D+07    |proj g|=  8.88889D-01\n",
      "\n",
      "At iterate    3    f=  1.16961D+06    |proj g|=  1.21318D-01\n",
      "\n",
      "At iterate    6    f=  1.12277D+06    |proj g|=  9.16120D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20329/1123782311.py:64: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  self.res = minimize(self.total_loss, v_0,  args=(args),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9      7      9     12     0     3   3.260D-01   1.123D+06\n",
      "  F =   1122767.8429995503     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.61512D+06    |proj g|=  9.97569D-01\n",
      "\n",
      "At iterate    3    f=  1.40677D+06    |proj g|=  9.98923D-01\n",
      "\n",
      "At iterate    6    f=  1.40631D+06    |proj g|=  9.98922D-01\n",
      "  ys=-4.290E+01  -gs= 4.073E+03 BFGS update SKIPPED\n",
      "  ys=-3.550E-01  -gs= 2.373E+02 BFGS update SKIPPED\n",
      "\n",
      "At iterate    9    f=  1.40197D+06    |proj g|=  9.33283D-01\n",
      "  ys=-4.524E+03  -gs= 6.238E+00 BFGS update SKIPPED\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   12    f=  1.11947D+06    |proj g|=  9.98922D-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " Warning:  more than 10 function and gradient\n",
      "   evaluations in the last line search.  Termination\n",
      "   may possibly be caused by a bad search direction.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   15    f=  1.11947D+06    |proj g|=  1.16415D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9     15    128     23     3     0   1.164D-01   1.119D+06\n",
      "  F =   1119470.9020986350     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.70224D+06    |proj g|=  9.99925D-01\n",
      "\n",
      "At iterate    3    f=  1.12277D+06    |proj g|=  9.86864D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9      5     13     11     0     0   1.441D-01   1.123D+06\n",
      "  F =   1122767.8429995468     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.53762D+06    |proj g|=  9.78775D-01\n",
      "\n",
      "At iterate    3    f=  1.13424D+06    |proj g|=  9.47388D-01\n",
      "\n",
      "At iterate    6    f=  1.12277D+06    |proj g|=  9.32432D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9      8     12     16     0     0   9.324D-01   1.123D+06\n",
      "  F =   1122767.8429996523     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.08938D+07    |proj g|=  9.76605D-01\n",
      "\n",
      "At iterate    3    f=  1.12277D+06    |proj g|=  9.17681D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9      5     11     10     0     0   9.160D-02   1.123D+06\n",
      "  F =   1122767.8429995475     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.93531D+07    |proj g|=  9.38370D-01\n",
      "\n",
      "At iterate    3    f=  1.12277D+06    |proj g|=  9.14678D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9      4     11     10     0     1   4.657D-02   1.123D+06\n",
      "  F =   1122767.8429995480     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.02036D+07    |proj g|=  9.39841D-01\n",
      "\n",
      "At iterate    3    f=  1.12277D+06    |proj g|=  9.13567D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9      4     10      9     0     0   6.985D-02   1.123D+06\n",
      "  F =   1122767.8429995480     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.10694D+07    |proj g|=  9.79367D-01\n",
      "\n",
      "At iterate    3    f=  1.12277D+06    |proj g|=  9.15535D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9      4      9      9     0     1   9.085D-01   1.123D+06\n",
      "  F =   1122767.8429995663     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    }
   ],
   "source": [
    "result.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1119470.90209863]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[117527.82333086]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.original_data.min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>pidp</th>\n",
       "      <th>asian</th>\n",
       "      <th>black</th>\n",
       "      <th>dvage</th>\n",
       "      <th>ind_inc_deflated</th>\n",
       "      <th>low</th>\n",
       "      <th>mastat_recoded</th>\n",
       "      <th>middle</th>\n",
       "      <th>mixed</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>68571207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>2011.0</td>\n",
       "      <td>68571207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>2012.0</td>\n",
       "      <td>68571207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>68571207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>2014.0</td>\n",
       "      <td>68571207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>2014.0</td>\n",
       "      <td>81959733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>968.718466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>81959733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>81959733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>81959733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>81959733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>909 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        year      pidp  asian  black  dvage  ind_inc_deflated  low  \\\n",
       "603   2010.0  68571207    0.0    0.0   70.0          0.000000  1.0   \n",
       "604   2011.0  68571207    0.0    0.0   71.0          0.000000  1.0   \n",
       "605   2012.0  68571207    0.0    0.0   72.0          0.000000  1.0   \n",
       "606   2013.0  68571207    0.0    0.0   73.0          0.000000  1.0   \n",
       "607   2014.0  68571207    0.0    0.0   75.0          0.000000  1.0   \n",
       "...      ...       ...    ...    ...    ...               ...  ...   \n",
       "1480  2014.0  81959733    0.0    0.0   23.0        968.718466  0.0   \n",
       "1481  2015.0  81959733    0.0    0.0   24.0          0.000000  0.0   \n",
       "1482  2016.0  81959733    0.0    0.0   25.0          0.000000  0.0   \n",
       "1483  2017.0  81959733    0.0    0.0   26.0          0.000000  0.0   \n",
       "1484  2018.0  81959733    0.0    0.0   27.0          0.000000  0.0   \n",
       "\n",
       "      mastat_recoded  middle  mixed  other  \n",
       "603              1.0     0.0    0.0    0.0  \n",
       "604              1.0     0.0    0.0    0.0  \n",
       "605              1.0     0.0    0.0    0.0  \n",
       "606              1.0     0.0    0.0    0.0  \n",
       "607              1.0     0.0    0.0    0.0  \n",
       "...              ...     ...    ...    ...  \n",
       "1480             1.0     0.0    0.0    0.0  \n",
       "1481             1.0     0.0    0.0    0.0  \n",
       "1482             1.0     0.0    0.0    0.0  \n",
       "1483             1.0     0.0    0.0    0.0  \n",
       "1484             1.0     0.0    0.0    0.0  \n",
       "\n",
       "[909 rows x 11 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pidp</th>\n",
       "      <th>81959733</th>\n",
       "      <th>1156396447</th>\n",
       "      <th>1157758495</th>\n",
       "      <th>1224193807</th>\n",
       "      <th>1224399847</th>\n",
       "      <th>1292191091</th>\n",
       "      <th>1360093847</th>\n",
       "      <th>1360438607</th>\n",
       "      <th>1360828247</th>\n",
       "      <th>136330499</th>\n",
       "      <th>...</th>\n",
       "      <th>884697011</th>\n",
       "      <th>885176407</th>\n",
       "      <th>885415767</th>\n",
       "      <th>885667371</th>\n",
       "      <th>885797927</th>\n",
       "      <th>89037165</th>\n",
       "      <th>952716047</th>\n",
       "      <th>953915567</th>\n",
       "      <th>953924411</th>\n",
       "      <th>96145209</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">asian</th>\n",
       "      <th>2010.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">other</th>\n",
       "      <th>2014.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "pidp          81959733  1156396447  1157758495  1224193807  1224399847  \\\n",
       "      year                                                               \n",
       "asian 2010.0       0.0         0.0         1.0         0.0         0.0   \n",
       "      2011.0       0.0         0.0         1.0         0.0         0.0   \n",
       "      2012.0       0.0         0.0         1.0         0.0         0.0   \n",
       "      2013.0       0.0         0.0         1.0         0.0         0.0   \n",
       "      2014.0       0.0         0.0         1.0         0.0         0.0   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "other 2014.0       0.0         0.0         0.0         0.0         0.0   \n",
       "      2015.0       0.0         0.0         0.0         0.0         0.0   \n",
       "      2016.0       0.0         0.0         0.0         0.0         0.0   \n",
       "      2017.0       0.0         0.0         0.0         0.0         0.0   \n",
       "      2018.0       0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "pidp          1292191091  1360093847  1360438607  1360828247  136330499  ...  \\\n",
       "      year                                                               ...   \n",
       "asian 2010.0         1.0         0.0         0.0         0.0        0.0  ...   \n",
       "      2011.0         1.0         0.0         0.0         0.0        0.0  ...   \n",
       "      2012.0         1.0         0.0         0.0         0.0        0.0  ...   \n",
       "      2013.0         1.0         0.0         0.0         0.0        0.0  ...   \n",
       "      2014.0         1.0         0.0         0.0         0.0        0.0  ...   \n",
       "...                  ...         ...         ...         ...        ...  ...   \n",
       "other 2014.0         0.0         0.0         0.0         0.0        0.0  ...   \n",
       "      2015.0         0.0         0.0         0.0         0.0        0.0  ...   \n",
       "      2016.0         0.0         0.0         0.0         0.0        0.0  ...   \n",
       "      2017.0         0.0         0.0         0.0         0.0        0.0  ...   \n",
       "      2018.0         0.0         0.0         0.0         0.0        0.0  ...   \n",
       "\n",
       "pidp          884697011  885176407  885415767  885667371  885797927  89037165  \\\n",
       "      year                                                                      \n",
       "asian 2010.0        0.0        0.0        1.0        0.0        0.0       0.0   \n",
       "      2011.0        0.0        0.0        1.0        0.0        0.0       0.0   \n",
       "      2012.0        0.0        0.0        1.0        0.0        0.0       0.0   \n",
       "      2013.0        0.0        0.0        1.0        0.0        0.0       0.0   \n",
       "      2014.0        0.0        0.0        1.0        0.0        0.0       0.0   \n",
       "...                 ...        ...        ...        ...        ...       ...   \n",
       "other 2014.0        0.0        0.0        0.0        0.0        0.0       0.0   \n",
       "      2015.0        0.0        0.0        0.0        0.0        0.0       0.0   \n",
       "      2016.0        0.0        0.0        0.0        0.0        0.0       0.0   \n",
       "      2017.0        0.0        0.0        0.0        0.0        0.0       0.0   \n",
       "      2018.0        0.0        0.0        0.0        0.0        0.0       0.0   \n",
       "\n",
       "pidp          952716047  953915567  953924411  96145209  \n",
       "      year                                               \n",
       "asian 2010.0        0.0        1.0        1.0       0.0  \n",
       "      2011.0        0.0        1.0        1.0       0.0  \n",
       "      2012.0        0.0        1.0        1.0       0.0  \n",
       "      2013.0        0.0        1.0        1.0       0.0  \n",
       "      2014.0        0.0        1.0        1.0       0.0  \n",
       "...                 ...        ...        ...       ...  \n",
       "other 2014.0        0.0        0.0        0.0       0.0  \n",
       "      2015.0        0.0        0.0        0.0       0.0  \n",
       "      2016.0        0.0        0.0        0.0       0.0  \n",
       "      2017.0        0.0        0.0        0.0       0.0  \n",
       "      2018.0        0.0        0.0        0.0       0.0  \n",
       "\n",
       "[81 rows x 101 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_object['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "costofcare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
